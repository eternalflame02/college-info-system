# MBCET CSE Semantic Chunking Pipeline

A Python-based pipeline for scraping, processing, and semantically chunking MBCET CSE department content for RAG (Retrieval-Augmented Generation) and Knowledge Graph applications.

![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Pipeline Stages](#-pipeline-stages)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Output Formats](#-output-formats)
- [Testing](#-testing)
- [Contributing](#-contributing)

---

## âœ¨ Features

- **Web Scraping**: Crawls MBCET website with intelligent URL discovery
- **PDF Processing**: Extracts text and tables from syllabus PDFs using `pdfplumber`
- **OCR Support**: Handles scanned documents with Tesseract OCR
- **Table Extraction**: Preserves tabular data as structured Markdown tables
- **Entity Registry**: Extracts and normalizes faculty names with aliases
- **Semantic Chunking**: Creates semantically meaningful chunks for RAG
- **Duplicate Detection**: SHA-256 based deduplication

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MBCET Website                            â”‚
â”‚                    (mbcet.ac.in/cse)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     URL Discovery                               â”‚
â”‚              (Crawls pages, identifies PDFs)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTML Scraper  â”‚   â”‚ PDF Handler   â”‚
â”‚ (markdownify) â”‚   â”‚ (pdfplumber)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Markdown Files                               â”‚
â”‚                  (data/markdown/)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Entity Registryâ”‚   â”‚   Semantic    â”‚
â”‚   Builder     â”‚   â”‚   Chunker     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ faculty.json  â”‚   â”‚ chunks.json   â”‚
â”‚ courses.json  â”‚   â”‚ 640 chunks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.11 or higher
- Tesseract OCR (for scanned PDF support)
- Git

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/mbcet-chunking-pipeline.git
cd mbcet-chunking-pipeline
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv

# Windows
.\venv\Scripts\activate

# Linux/macOS
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Install Tesseract OCR (Optional - for scanned PDFs)

**Windows:**
- Download from: https://github.com/UB-Mannheim/tesseract/wiki
- Add to PATH or set in `.env`

**Linux:**
```bash
sudo apt-get install tesseract-ocr
```

**macOS:**
```bash
brew install tesseract
```

### Step 5: Configure Environment

```bash
cp .env.example .env
# Edit .env with your settings
```

---

## ğŸ“– Usage

### Run Complete Pipeline

```bash
python main.py --stage all
```

### Run Individual Stages

```bash
# Stage 1: Scrape website and PDFs
python main.py --stage scrape

# Stage 2: Build entity registry
python main.py --stage entities

# Stage 3: Run semantic chunker
python main.py --stage chunk
```

### Verbose Mode

```bash
python main.py --stage all -v
```

---

## ğŸ”„ Pipeline Stages

### 1. Web Scraping (`--stage scrape`)

**Purpose:** Crawls MBCET CSE website and converts content to Markdown.

**What it does:**
- Discovers all pages under `/cse` using BFS crawling
- Identifies PDF links (syllabi, regulations)
- Converts HTML pages to clean Markdown
- Extracts tables from PDFs using `pdfplumber`
- Falls back to OCR for scanned documents

**Output:**
- `data/markdown/pages/` - 60+ HTML-derived Markdown files
- `data/markdown/pdfs/` - 10 PDF-derived Markdown files

### 2. Entity Registry (`--stage entities`)

**Purpose:** Extracts named entities for knowledge graph construction.

**What it does:**
- Parses faculty list from department page
- Normalizes names (removes titles like Dr., Prof.)
- Generates aliases for entity linking
- Creates unique entity IDs

**Output:**
- `data/entities/faculty.json` - 43 faculty entities
- `data/entities/courses.json` - Course entities (planned)
- `data/entities/programs.json` - Program entities (planned)

### 3. Semantic Chunking (`--stage chunk`)

**Purpose:** Splits documents into semantically meaningful chunks.

**What it does:**
- Parses Markdown structure (headers, lists, tables, paragraphs)
- Classifies content types (profile, regulation, table, section, list)
- Links chunks to entities (faculty references)
- Detects and skips duplicates (SHA-256 hashing)
- Tracks page ranges for PDF sources

**Output:**
- `data/chunks/chunks.json` - 640 semantic chunks
- `data/chunks/chunk_report.json` - Statistics and summary

---

## ğŸ“ Project Structure

```
mbcet-chunking-pipeline/
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ config.py               # Configuration and paths
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment template
â”‚
â”œâ”€â”€ scraper/                # Web scraping module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ url_discovery.py    # BFS URL crawler
â”‚   â”œâ”€â”€ html_scraper.py     # HTML to Markdown converter
â”‚   â”œâ”€â”€ pdf_handler.py      # PDF processing with pdfplumber
â”‚   â””â”€â”€ markdown_converter.py # HTML cleaning and conversion
â”‚
â”œâ”€â”€ chunker/                # Semantic chunking module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ semantic_chunker.py # Main chunking logic
â”‚   â”œâ”€â”€ entity_registry.py  # Entity extraction and normalization
â”‚   â””â”€â”€ chunk_classifiers.py # Content type classification
â”‚
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â””â”€â”€ test_entities.py
â”‚
â”œâ”€â”€ data/                   # Generated data (gitignored)
â”‚   â”œâ”€â”€ raw/                # Downloaded PDFs
â”‚   â”œâ”€â”€ markdown/           # Converted Markdown files
â”‚   â”‚   â”œâ”€â”€ pages/          # From HTML pages
â”‚   â”‚   â””â”€â”€ pdfs/           # From PDF documents
â”‚   â”œâ”€â”€ entities/           # Entity registries
â”‚   â””â”€â”€ chunks/             # Final chunks
â”‚
â””â”€â”€ Docs/                   # Reference documents
```

---

## âš™ï¸ Configuration

Configuration is managed via `config.py` and `.env`:

### Environment Variables (`.env`)

```bash
# Base URL for scraping
BASE_URL=https://mbcet.ac.in
CSE_DEPARTMENT_URL=https://mbcet.ac.in/cse

# Request settings
REQUEST_TIMEOUT=30
REQUEST_DELAY=1.0
MAX_RETRIES=3

# Tesseract path (Windows)
TESSERACT_CMD=C:/Program Files/Tesseract-OCR/tesseract.exe
```

### Chunking Settings (`config.py`)

| Setting | Default | Description |
|---------|---------|-------------|
| `MAX_CHUNK_WORDS` | 500 | Maximum words per chunk |
| `MIN_CHUNK_WORDS` | 50 | Minimum words per chunk |
| `OVERLAP_SENTENCES` | 2 | Sentence overlap between chunks |

---

## ğŸ“¤ Output Formats

### Chunks JSON Schema

```json
{
  "chunk_id": "pdf_cse_syllabus_root_a1b2c3d4",
  "text": "## Data Structures\n\nModule 1: Arrays and linked lists...",
  "source_type": "pdf",
  "source_file": "data/markdown/pdfs/CSE_Syllabus.md",
  "section_hierarchy": ["Semester 3", "Data Structures"],
  "content_type": "regulation",
  "entity_refs": ["faculty_dr_john_doe"],
  "page_range": [15, 17],
  "word_count": 245,
  "hash": "sha256:abc123..."
}
```

### Entity JSON Schema

```json
{
  "id": "faculty_dr_john_doe",
  "name": "Dr. John Doe",
  "aliases": ["John Doe", "Dr John Doe"],
  "type": "faculty",
  "url": "https://mbcet.ac.in/cse/faculty/john-doe"
}
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
pytest
```

### Run with Coverage

```bash
pytest --cov=scraper --cov=chunker --cov-report=html
```

### Run Specific Tests

```bash
pytest tests/test_scraper.py -v
pytest tests/test_chunker.py -v
```

---

## ğŸ”§ Key Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `requests` | â‰¥2.31.0 | HTTP client |
| `beautifulsoup4` | â‰¥4.12.0 | HTML parsing |
| `markdownify` | â‰¥0.12.0 | HTML to Markdown |
| `pdfplumber` | â‰¥0.11.0 | PDF table extraction |
| `pypdf` | â‰¥4.0.0 | PDF text extraction |
| `pytesseract` | â‰¥0.3.10 | OCR integration |
| `pyyaml` | â‰¥6.0.0 | YAML frontmatter |
| `tqdm` | â‰¥4.66.0 | Progress bars |
| `pytest` | â‰¥8.0.0 | Testing framework |

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| Total pages scraped | 60 |
| Total PDFs processed | 10 |
| Chunks generated | 640 |
| Duplicates detected | 1 |
| Faculty entities | 43 |
| Test coverage | 50 tests passing |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Authors

- **Rohith** - *Initial work* - MBCET CSE Department

---

## ğŸ™ Acknowledgments

- MBCET CSE Department for the source content
- pdfplumber team for excellent table extraction
- Tesseract OCR community
